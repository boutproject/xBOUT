{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the loading function from xBOUT\n",
    "from xbout import open_boutdataset\n",
    "\n",
    "# Import xarray, which is the real hero here\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you have already run the `delta_1` case of the `blob2d` [example](https://github.com/boutproject/BOUT-dev/tree/master/examples/blob2d) from the BOUT++ repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the data and see what's inside\n",
    "ds = open_boutdataset('./delta_1/BOUT.dmp.*.nc', inputfilepath='./delta_1/BOUT.inp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all the variables, but none of them have been loaded yet, thanks to xarray's \"lazy loading\".\n",
    "\n",
    "Each variable depends on dimensions or `dims`, which are like axes of numpy arrays but labelled with a name.\n",
    "\n",
    "The `Dataset` is like a dictionary container of multiple `DataArrays`, each of which represents a simulation variable.\n",
    "\n",
    "We have also stored various unphysical simulation quantities (such as processor splitting) in the `attrs` of the datasets, which is just a dictionary for carrying arbitrary extra info about the dataset's contents. \n",
    "\n",
    "Because we specified the path to the input file, the run options have also been included as a `ConfigParser` object.\n",
    "\n",
    "Currently the dataset has no coordinates, as the warning mentions, but we will come back to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object we've loaded is an `xarray.Dataset`, filled out in a sensible way for BOUT++ data. One way to think of it is as an in-memory representation of a netCDF file. Another way is like a set of numpy arrays with labelled axes. (If you've used pandas then it's also like a multidimensional pandas series.)\n",
    "\n",
    "You will find the [xarray documentation](https://xarray.pydata.org/en/stable/index.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see we have an unneccessary y dimension of length 1, so let's drop that\n",
    "ds = ds.squeeze(drop=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xarray is just wrapping numpy arrays, so we can always get our values back if we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['t_array'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic plotting with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like? Let's try plotting a single frame\n",
    "# Choose the density, at the 10th time index\n",
    "plt.figure()\n",
    "ds['n'].isel(t=10).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's a blob!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note xarray's `.plot()` method just wraps matplotlib, so you can pass matplotlib commands straight to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ds['n'].isel(t=10).plot(cmap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's watch it fly\n",
    "ds['n'].bout.animate2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(xarray plots do not open a new figure, so we need to create a new one first. The ds.bout plotting and animation methods do create a new figure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's animate everything\n",
    "ds.bout.animate_list(['n', 'phi', 'omega'], nrows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swirly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some coordinates for the radial and binormal directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dz is a scalar, so gets stored in 'metadata'\n",
    "dz = xr.DataArray(ds.metadata['dz']).expand_dims({'z': ds.dims['z']})\n",
    "z = dz.cumsum(dim='z')\n",
    "ds = ds.assign_coords({'z': z})\n",
    "\n",
    "# We already have dx, so let's use that\n",
    "x = ds['dx'].cumsum(dim='x')\n",
    "ds = ds.assign_coords({'x': x})\n",
    "\n",
    "# The time array is also already in the data\n",
    "ds = ds.assign_coords({'t': ds['t_array']})\n",
    "\n",
    "# Now we have some coordinates!\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Aside:) It would be nice to do all that every time we open our data, e.g.\n",
    "def open_blob2ddataset(datapath='./BOUT.dmp.*', inputfilepath='./BOUT.inp'):\n",
    "    ds = open_boutdataset(datapath=datapath, inputfilepath=inputfilepath).squeeze(drop=True)\n",
    "\n",
    "    # add your physics-model-specific coordinates here\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates (or `coords`) are basically just data variables that have been given special status.\n",
    "\n",
    "However, they can also be used to index the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['n'].isel(t=10)  # selects the 10th slice along the t dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['n'].sel(t=50)  # selects the slice which has a t value of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have scipy installed, you can also interpolate the data values\n",
    "ds['n'].interp(t=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The metadata can be read for plotting\n",
    "ds['t'].attrs['units'] = '1/wci'\n",
    "ds['x'].attrs['units'] = 'rhos'\n",
    "ds['z'].attrs['units'] = 'rhos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (full integration of BoutOptionsFile and xBOUT will come...)\n",
    "# note BOUT.settings stores all the options actually used in a run in input-file format\n",
    "from boutdata.data import BoutOptionsFile\n",
    "options = BoutOptionsFile('./delta_1/BOUT.settings')\n",
    "\n",
    "# We can un-normalise our data, so it's in physical units\n",
    "n0 = options['model']['n0']\n",
    "ds['n'] = ds['n'] * n0\n",
    "ds['n'].attrs['units'] = 'm-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we plot again we will see the units too\n",
    "plt.figure()\n",
    "ds['n'].isel(t=10).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's do some physics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the velocity of the centre-of-mass of just the filament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the filament, defined as the region with density greater than some multiple of the background\n",
    "threshold = 1.1 * n0\n",
    "blob = ds.where(ds['n'] > threshold)\n",
    "\n",
    "# Now all the rest of the data has been replaced with NaNs\n",
    "# xarray will exclude the NaNs when plotting\n",
    "plt.figure()\n",
    "blob['n'].isel(t=10).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CoM\n",
    "n, x = ds['n'], ds['x']\n",
    "\n",
    "ntotal = n.sum(dim=['x','z'])\n",
    "xCoM = (x*n).sum(dim=['x','z']) / ntotal\n",
    "zCoM = (z*n).sum(dim=['x','z']) / ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find velocity of CoM\n",
    "v_xCoM = xCoM.differentiate('t')\n",
    "v_zCoM = zCoM.differentiate('t')\n",
    "\n",
    "# This quantity is 1D - but xarray knows to use a line plot to plot it \n",
    "plt.figure()\n",
    "v_xCoM.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contours of electric potential on top of vorticity\n",
    "plt.figure()\n",
    "ds['omega'].isel(t=15).plot()\n",
    "ds['phi'].isel(t=15).plot.contour(center=0.0, cmap='seismic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xBOUT's calc module\n",
    "\n",
    "`xBOUT` is also a good place to store analysis methods and functions which are likely to be useful to many BOUT++ users. These should go in the `calc` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing multiple runs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the results from multiple simulation runs into a single dataset.\n",
    "# This is great for analysing parameter scans\n",
    "widths = [0.25, 1, 10]\n",
    "runs = []\n",
    "for w in widths:\n",
    "    run = open_blob2ddataset(datapath=f'./delta_{w}/BOUT.dmp.*.nc', inputfilepath=f'./delta_{w}/BOUT.inp')\n",
    "    runs.append(run)\n",
    "width_coord = xr.DataArray(amplitudes, dims='w')\n",
    "scan = xr.concat(runs, dim=width_coord)\n",
    "print(scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this isn't going to work with the standard blob2d example data.\n",
    "That's because the default options for the three cases produce time series of different lengths.\n",
    "\n",
    "You will have to re-run the simulations with the same time resolution and length if you want to combine them like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the velocities separately then join together for plotting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all the runs together, we can compare them easily\n",
    "# Plot of average velocities against amplitude?\n",
    "# Compare to analytic scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-model-specific accessors\n",
    "\n",
    "`xbout` achieves the `ds.bout.method()` syntax by using the [\"accessor\"](https://xarray.pydata.org/en/stable/internals.html#extending-xarray) interface provided by xarray.\n",
    "\n",
    "This is great because it allows us to attach domain specific functionality (i.e. tokamak-specific plotting methods) to general data structures (i.e. `xarray.Dataset` objects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go further though.\n",
    "\n",
    "The accessor classes `BoutDatasetAccessor` and `BoutDataArrayAccessor` are intended to be subclassed for specific BOUT++ modules. \n",
    "The subclass accessor will then inherit all the `.bout` accessor methods, but you will also be able to override these and define your own methods within your new accessor.\n",
    "\n",
    "For example to add an extra method specific to the STORM BOUT++ module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xarray import register_dataset_accessor\n",
    "from xbout.boutdataset import BoutDatasetAccessor\n",
    "\n",
    "@register_dataset_accessor('storm')\n",
    "class StormAccessor(BoutDatasetAccessor):\n",
    "    def __init__(self, ds_object):\n",
    "        super().__init__(ds_object)\n",
    "\n",
    "    def special_method(self):\n",
    "        print(\"Do something only STORM users would want to do\")\n",
    "\n",
    "ds.storm.special_method()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using accessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xarray import register_dataset_accessor\n",
    "\n",
    "from xbout import BoutDatasetAccessor, BoutDataArrayAccessor\n",
    "\n",
    "\n",
    "@register_dataset_accessor('utils')\n",
    "class UtilityDatasetAccessor(BoutDatasetAccessor):\n",
    "    \"\"\"\n",
    "    Class specifically for calculating ExB velocities of BOUT++ data.\n",
    "    \n",
    "    Requires that the BOUT++ data has a 'phi' field and 'x' and 'z' coordinates,\n",
    "    in addition to the default 'Bxy' magnetic field.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ds):\n",
    "        super().__init__(ds)\n",
    "        self.data = ds\n",
    "\n",
    "    # This is where module-specific methods would go\n",
    "    # For example maybe elm-pb would have a .elm_growth_rate() method?\n",
    "\n",
    "    @property\n",
    "    def v_radial(self):\n",
    "        \"\"\"Calculates local radial ExB velocity\"\"\"\n",
    "\n",
    "        if 'v_radial' not in self.data:\n",
    "            E_z = self.data['phi'].differentiate(coord='z')\n",
    "            v_radial = E_z / self.data['Bxy']\n",
    "            v_radial.attrs['standard_name'] = 'radial velocity'\n",
    "            self.data['v_radial'] = v_radial\n",
    "        return self.data['v_radial']\n",
    "\n",
    "    @property\n",
    "    def v_binormal(self):\n",
    "        \"\"\"Calculates local binormal ExB velocity\"\"\"\n",
    "\n",
    "        if 'v_binormal' not in self.data:\n",
    "            E_x = self.data['phi'].differentiate(coord='x')\n",
    "            v_binormal = -E_x / self.data['Bxy']\n",
    "            v_binormal.attrs['standard_name'] = 'binormal velocity'\n",
    "            self.data['v_binormal'] = v_binormal\n",
    "        return self.data['v_binormal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've written the methods in like this (using the property decorator) so that they can be calculated like variables which already exist, and then saved on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a time slice, downsample, and select region of interest\n",
    "bd = ds.isel(t=10, x=slice(50, 150, 3), z=slice(75, 175, 3))\n",
    "\n",
    "# Find ExB velocities\n",
    "vx = bd.utils.v_radial\n",
    "vz = bd.utils.v_binormal\n",
    "\n",
    "bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot streamlines\n",
    "fig, ax = plt.subplots()\n",
    "bd['phi'].transpose().plot.contour(center=0.0, cmap='seismic', ax=ax)\n",
    "\n",
    "# Plot the flow\n",
    "# (we're using pure matplotlib so have to match up the dimensions)\n",
    "x, z = bd['x'].broadcast_like(vx), bd['z'].broadcast_like(vx)\n",
    "ax.quiver(x.values, z.values, vx.values, vz.values, scale=1.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your own data\n",
    "\n",
    "If you have some of your own data, try loading that.\n",
    "For 3D tokamak datasets specify `geometry='toroidal'` in `open_boutdataset`.\n",
    "Then when plotting try `ds[var].bout.pcolormesh()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
